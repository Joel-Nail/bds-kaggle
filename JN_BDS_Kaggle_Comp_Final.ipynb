{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created by: Joel Nail\n",
    "\n",
    "#### Goal: achieve the highest possible AUC score for the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the datasets\n",
    "train_df = pd.read_csv(\"data/train_final.csv\")\n",
    "test_df = pd.read_csv(\"data/test_final.csv\")\n",
    "\n",
    "# isolating X and y from training set and renaming the test set\n",
    "X = train_df.drop(labels='Y', axis='columns')\n",
    "y = train_df['Y']\n",
    "X_kag_test = test_df\n",
    "\n",
    "# splitting my overall training set into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that accepts a desired output filename and and prediction proba array to create a csv for submission\n",
    "def write_preds_to_csv(target_filename, model_preds_proba):\n",
    "    X = train_df.drop(labels='Y', axis='columns')\n",
    "    y = train_df['Y']\n",
    "    X_kag_test = test_df\n",
    "\n",
    "    #### I just realized I could just take the prediction in the 1st index of model_test\n",
    "    #### that will always be the value we want to report - the code below is convoluted and unnecesary (but it does work)\n",
    "    #### I guess it does provide a nice way of understanding how the proba function works, but still unecessarily long\n",
    "\n",
    "    model_test_preds = []\n",
    "    for pair in model_preds_proba:\n",
    "        pair_list = pair.tolist()\n",
    "        highest_prob = max(pair_list)\n",
    "        index = pair_list.index(highest_prob)\n",
    "        #print(index)\n",
    "        if index == 0:\n",
    "            model_test_preds.append(1-highest_prob)\n",
    "        else:\n",
    "            model_test_preds.append(highest_prob)\n",
    "\n",
    "    pred_df = pd.DataFrame(model_test_preds)\n",
    "    i=0\n",
    "    n = 2604\n",
    "    preds = []\n",
    "    for pair in pred_df.values:\n",
    "        preds.append([n, np.max(pair)])\n",
    "        i+=1\n",
    "        n+=1\n",
    "    #print(preds)\n",
    "    pred_df = pd.DataFrame(preds, columns=[\"Id\", \"Y\"])\n",
    "    #print(pred_df)\n",
    "    file_name = target_filename + \".csv\"\n",
    "    pred_df.to_csv(\"/Users/joelnail/Documents/BDS/Kaggle_Comp/Predictions/\" + file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code I used for testing my models prior to submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model on public leaderboard (random_strength = 0.25)\n",
    "cat_model = cb.CatBoostClassifier(learning_rate=.01, l2_leaf_reg=1, iterations=1000, depth=5, border_count=33,\\\n",
    "    boosting_type='Ordered', random_strength=.25)\n",
    "\n",
    "# best model on private leaderboard (random_strength = 0.5) \n",
    "# unfortunately, I did not choose this model to be considered in my final score - oh well\n",
    "cat_model = cb.CatBoostClassifier(learning_rate=.01, l2_leaf_reg=1, iterations=1000, depth=5, border_count=33,\\\n",
    "    boosting_type='Ordered', random_strength=.5)\n",
    "\n",
    "cat_model.fit(X_train, y_train, verbose=250)\n",
    "cat_model_preds = cat_model.predict_proba(X_test)\n",
    "\n",
    "cat_preds_proba = []\n",
    "for pred in cat_model_preds:\n",
    "   cat_preds_proba.append(pred[1])\n",
    "\n",
    "print(roc_auc_score(y_test, cat_preds_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to fit the model to the entire training set and create the prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = cb.CatBoostClassifier(learning_rate=.01, l2_leaf_reg=1, iterations=1000, depth=5, border_count=33,\\\n",
    "    boosting_type='Ordered', random_strength=.5)\n",
    "\n",
    "cat_model.fit(X, y, verbose=250)\n",
    "cat_model_preds = cat_model.predict_proba(X_kag_test)\n",
    "write_preds_to_csv(\"best_model_preds\", cat_model_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code I used to find the best CatBoost hyperparameters. Grid Search took too long, so I primarily used Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = cb.CatBoostClassifier()\n",
    "\n",
    "params = {'depth':[1,2,3,4,5,6,7,8,9,10],\n",
    "          'iterations':[250,500,1000],\n",
    "          'learning_rate':[0.01,0.1,0.2,0.3], \n",
    "          'l2_leaf_reg':[1,3,5,10,100],\n",
    "          'border_count':[5,10,20,32,50,100,200]}\n",
    "\n",
    "params2 = {'depth':[5],\n",
    "          'iterations':[1000],\n",
    "          'learning_rate':[0.01], \n",
    "          'l2_leaf_reg':[1],\n",
    "          'border_count':[32],\n",
    "          'bootstrap_type':['MVS'],\n",
    "          'boosting_type':['Ordered', 'Plain']}\n",
    "\n",
    "# grid = GridSearchCV(estimator=cat_model, param_grid = params2, cv = 2, scoring=\"roc_auc\")\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# print(\"RESULTS\")\n",
    "# print(\"\\n The best estimator across ALL searched params:\\n\", grid.best_estimator_)\n",
    "# print(\"\\n The best score across ALL searched params:\\n\", grid.best_score_)\n",
    "# print(\"\\n The best parameters across ALL searched params:\\n\", grid.best_params_)\n",
    "\n",
    "rand_search = RandomizedSearchCV(estimator=cat_model, param_distributions=params2, scoring=\"roc_auc\", verbose=0, n_iter=20)\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"RESULTS\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", rand_search.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", rand_search.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Catboost hyperparameter tuning gave me the best results, I did attempt stacking. If I'd had more time (i.e., started earlier haha), I think I could've produced a stacking model that outperformed my best model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "cat_model = cb.CatBoostClassifier(learning_rate=.01, l2_leaf_reg=1, iterations=1000, depth=5, border_count=32, boosting_type='Ordered')\n",
    "xgb_model = xgb.XGBClassifier(min_child_weight=3, max_depth=4, learning_rate=0.05, gamma=0.2, colsample_bytree=0.7)\n",
    "xgbrf_model = xgb.XGBRFClassifier()\n",
    "\n",
    "level0 = list()\n",
    "#level0.append(('rf', RandomForestClassifier()))\n",
    "#level0.append(('dt', DecisionTreeClassifier()))\n",
    "level0.append(('xgb', xgb_model))\n",
    "level0.append(('xgbrf', xgbrf_model))\n",
    "level0.append(('cb', cat_model))\n",
    "\n",
    "stacked = StackingClassifier(estimators=level0, final_estimator=lr, cv=5, stack_method=\"predict_proba\")\n",
    "stacked.fit(X_train, y_train)\n",
    "stacked_preds = stacked.predict(X_test)\n",
    "print(accuracy_score(y_test, stacked_preds))\n",
    "\n",
    "stacked.fit(X, y)\n",
    "stacked_preds = stacked.predict_proba(X_kag_test)\n",
    "write_preds_to_csv(\"stack_v7\", stacked_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to CatBoost, I tried good ole logistic regression to gauge a baseline - it wasn't very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', C=10, random_state=42, solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=10000)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "lr_preds = lr.predict_proba(X_test)\n",
    "\n",
    "lr_preds_proba = []\n",
    "for pred in lr_preds:\n",
    "   lr_preds_proba.append(pred[1])\n",
    "\n",
    "lr_auc = roc_auc_score(y_test, lr_preds_proba)\n",
    "print(format(lr_auc, \".2%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried out XGBoost (normal classifier and RF classifier) before moving on to CatBoost which seemed to perform best on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBClassifier(colsample_bytree = 0.5, gamma = 0, learning_rate = 0.05, max_depth = 20, reg_lambda = 0, scale_pos_weight = 1, subsample = 0.8)\n",
    "\n",
    "model_xgb.fit(X_train, y_train)\n",
    "model_xgb_pred = model_xgb.predict_proba(X_test)\n",
    "\n",
    "xgb_preds_proba = []\n",
    "for pred in model_xgb_pred:\n",
    "    xgb_preds_proba.append(pred[1])\n",
    "\n",
    "print(roc_auc_score(y_test, xgb_preds_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgbrf = xgb.XGBRFClassifier(colsample_bytree = 0.5, gamma = 1, learning_rate = 0.01, max_depth = 20, reg_lambda = 0, scale_pos_weight = 1, subsample = 0.8)\n",
    "\n",
    "model_xgbrf.fit(X_train, y_train)\n",
    "model_xgbrf_pred = model_xgbrf.predict_proba(X_test)\n",
    "\n",
    "xgbrf_preds_proba = []\n",
    "for pred in model_xgbrf_pred:\n",
    "    xgbrf_preds_proba.append(pred[1])\n",
    "\n",
    "print(roc_auc_score(y_test, xgbrf_preds_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried running a neural network, but it didn't produce great results. I would have liked to play around with it more (especially stacking CatBoost with a Neural Network), but there wasn't enough time to experiment with everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model gets validation accuracy upwards of .8\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(X_train.shape[1:]))) # batch normalization very important for this dataset\n",
    "model.add(Dense(32, input_shape=(X_train.shape[1:])))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(X_train.shape[1:]))) # batch normalization very important for this dataset\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid')) # make sure to use sigmoid, not softmax (because it's binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.RMSprop(learning_rate=.01),\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X, y, \n",
    "    #batch_size=batch, \n",
    "    epochs=10,\n",
    "    #steps_per_epoch=16,\n",
    "    validation_data=(X_test, y_test), # we'll use the test set to perform validation\n",
    "    validation_freq=1, # we want to validate using the test set on each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that I didn't really try during the competition was feature engineering. I honestly wasn't sure where to start with feature engineering, and I wasn't sure how much of an AUC increase it would lead to, so I focused on trying different models and hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58f68c893d27d44de5f329b5c78da41b4997af8d492e4e8bd7c62547c498f83b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
